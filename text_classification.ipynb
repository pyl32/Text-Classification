{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pste_VLE_mJ6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "from statistics import stdev, mean\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, Activation, Conv1D, MaxPooling1D, LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94_jacAq_mJ_"
      },
      "source": [
        "# Text Classification\n",
        "\n",
        "## Introduction\n",
        "This project build a classifier to analyze the sentiment of reviews using Keras1 and Python. The text data store in two folders: one folder involves positive reviews, and one folder involves negative reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW6mwYr2_mKC"
      },
      "source": [
        "## Data Exploration and Pre-processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln9LiksG_mKD"
      },
      "source": [
        "First, I use binary encoding for the sentiments , i.e y = 1 for positive sentiments and y = âˆ’1 for negative sentiments. Since the provided data are pretty clean, we can remove the punctuation and numbers from the data for further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x41o9vUu_mKE",
        "outputId": "04998d2a-f367-4c07-f7e2-f08c009caecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#pos = os.listdir(\"../data/pos\")\n",
        "#neg = os.listdir(\"../data/neg\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_folder = '/content/drive/My Drive/552project/data'\n",
        "\n",
        "pos = os.listdir(drive_folder + '/pos/')\n",
        "neg = os.listdir(drive_folder + '/neg/')\n",
        "\n",
        "data = []\n",
        "punc_dig = string.punctuation + string.digits\n",
        "\n",
        "for path in pos:\n",
        "    f = open(\"/content/drive/My Drive/552project/data/pos/\" + path)\n",
        "    review = f.readlines()\n",
        "    f.close()\n",
        "    for i in range(len(review)):\n",
        "        review[i] = review[i].translate(str.maketrans('', '', punc_dig))\n",
        "    data.append([int(path[2:5]), \" \".join(review), 1])\n",
        "\n",
        "for path in neg:\n",
        "    f = open(\"/content/drive/My Drive/552project/data/neg/\" + path)\n",
        "    review = f.readlines()\n",
        "    f.close()\n",
        "    for i in range(len(review)):\n",
        "        review[i] = review[i].translate(str.maketrans('', '', punc_dig))\n",
        "    data.append([int(path[2:5]), \" \".join(review), -1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpdC7Zmk_mKF"
      },
      "source": [
        "The name of each text file starts with cv number. Here, I split the data use text files 0-699 in each class for training and 700-999 for testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aX0ek1nd_mKF"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(data, columns = [\"num\", \"text\", \"class\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "quZ6InB6_mKG"
      },
      "outputs": [],
      "source": [
        "train = data[data[\"num\"] < 700]\n",
        "test = data[data[\"num\"] >= 700]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ukm985V_mKH"
      },
      "source": [
        "Count the number of unique words in the whole dataset (train + test) and print it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g81N6Cdp_mKH",
        "outputId": "9e14b0a3-8f12-490d-ace6-ad690fc4ebb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of unique words:  46830\n"
          ]
        }
      ],
      "source": [
        "# resource: https://stackoverflow.com/questions/18936957/count-distinct-words-from-a-pandas-data-frame\n",
        "\n",
        "results = set()\n",
        "data['text'].str.lower().str.split().apply(results.update)\n",
        "print(\"number of unique words: \", len(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX1ccCJ5_mKH"
      },
      "outputs": [],
      "source": [
        "#result = data.text.apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWE5pJWD_mKJ"
      },
      "source": [
        "Calculate the average review length and the standard deviation of review lengths. Report the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2kf6xce_mKJ"
      },
      "outputs": [],
      "source": [
        "#data0 = data.replace(\"\\n\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh-ayaAJ_mKK",
        "outputId": "6cbc9cf7-62ca-45d0-a001-952071b559e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average:  644.3555\n",
            "standard deviation:  285.0511431249635\n"
          ]
        }
      ],
      "source": [
        "review_len = []\n",
        "\n",
        "for i in data[\"text\"]:\n",
        "    review_len.append(len(i.split()))\n",
        "\n",
        "print(\"average: \", mean(review_len))\n",
        "print(\"standard deviation: \", stdev(review_len))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d39fBF5L_mKL"
      },
      "source": [
        "### Visualization\n",
        "Plot the histogram of review lengths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNTnzAx7_mKL",
        "outputId": "22e8866d-7fd5-40cc-8fcf-68623daf4e61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([ 75., 537., 743., 417., 136.,  53.,  23.,  13.,   0.,   3.]),\n",
              " array([  16. ,  250.7,  485.4,  720.1,  954.8, 1189.5, 1424.2, 1658.9,\n",
              "        1893.6, 2128.3, 2363. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.hist(review_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR3lrYJa_mKM"
      },
      "source": [
        "### NLP/Deep Learning\n",
        "To represent each text (= data point), there are many ways. In NLP/Deep Learning terminology, this task is called tokenization. It is common to represent text using popularity/ rank of words in text. The most common word in the text will be represented as 1, the second most common word will be represented as 2, etc. Tokenize each text document using this method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROiFkbf9_mKM",
        "outputId": "46d93122-50c1-451d-d1cb-bcd1be8b8593"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'a': 2,\n",
              " 'and': 3,\n",
              " 'of': 4,\n",
              " 'to': 5,\n",
              " 'is': 6,\n",
              " 'in': 7,\n",
              " 'that': 8,\n",
              " 'it': 9,\n",
              " 'as': 10,\n",
              " 'with': 11,\n",
              " 'for': 12,\n",
              " 'his': 13,\n",
              " 'this': 14,\n",
              " 'film': 15,\n",
              " 'but': 16,\n",
              " 'he': 17,\n",
              " 'i': 18,\n",
              " 'on': 19,\n",
              " 'are': 20,\n",
              " 'by': 21,\n",
              " 'be': 22,\n",
              " 'its': 23,\n",
              " 'an': 24,\n",
              " 'not': 25,\n",
              " 'one': 26,\n",
              " 'movie': 27,\n",
              " 'who': 28,\n",
              " 'from': 29,\n",
              " 'at': 30,\n",
              " 'was': 31,\n",
              " 'have': 32,\n",
              " 'has': 33,\n",
              " 'her': 34,\n",
              " 'you': 35,\n",
              " 'they': 36,\n",
              " 'all': 37,\n",
              " 'so': 38,\n",
              " 'like': 39,\n",
              " 'about': 40,\n",
              " 'out': 41,\n",
              " 'more': 42,\n",
              " 'when': 43,\n",
              " 'which': 44,\n",
              " 'their': 45,\n",
              " 'up': 46,\n",
              " 'or': 47,\n",
              " 'what': 48,\n",
              " 'some': 49,\n",
              " 'just': 50,\n",
              " 'if': 51,\n",
              " 'there': 52,\n",
              " 'she': 53,\n",
              " 'him': 54,\n",
              " 'into': 55,\n",
              " 'even': 56,\n",
              " 'only': 57,\n",
              " 'than': 58,\n",
              " 'no': 59,\n",
              " 'we': 60,\n",
              " 'good': 61,\n",
              " 'most': 62,\n",
              " 'time': 63,\n",
              " 'can': 64,\n",
              " 'will': 65,\n",
              " 'story': 66,\n",
              " 'films': 67,\n",
              " 'been': 68,\n",
              " 'would': 69,\n",
              " 'much': 70,\n",
              " 'also': 71,\n",
              " 'characters': 72,\n",
              " 'other': 73,\n",
              " 'get': 74,\n",
              " 'character': 75,\n",
              " 'do': 76,\n",
              " 'them': 77,\n",
              " 'very': 78,\n",
              " 'two': 79,\n",
              " 'first': 80,\n",
              " 'after': 81,\n",
              " 'see': 82,\n",
              " 'well': 83,\n",
              " 'because': 84,\n",
              " 'way': 85,\n",
              " 'make': 86,\n",
              " 'any': 87,\n",
              " 'does': 88,\n",
              " 'really': 89,\n",
              " 'had': 90,\n",
              " 'too': 91,\n",
              " 'while': 92,\n",
              " 'how': 93,\n",
              " 'little': 94,\n",
              " 'life': 95,\n",
              " 'where': 96,\n",
              " 'were': 97,\n",
              " 'plot': 98,\n",
              " 'off': 99,\n",
              " 'people': 100,\n",
              " 'movies': 101,\n",
              " 'then': 102,\n",
              " 'me': 103,\n",
              " 'could': 104,\n",
              " 'my': 105,\n",
              " 'bad': 106,\n",
              " 'scene': 107,\n",
              " 'never': 108,\n",
              " 'being': 109,\n",
              " 'these': 110,\n",
              " 'over': 111,\n",
              " 'best': 112,\n",
              " 'new': 113,\n",
              " 'doesnt': 114,\n",
              " 'many': 115,\n",
              " 'man': 116,\n",
              " 'scenes': 117,\n",
              " 'such': 118,\n",
              " 'dont': 119,\n",
              " 'know': 120,\n",
              " 'through': 121,\n",
              " 'hes': 122,\n",
              " 'great': 123,\n",
              " 'another': 124,\n",
              " 'here': 125,\n",
              " 'love': 126,\n",
              " 's': 127,\n",
              " 'action': 128,\n",
              " 'go': 129,\n",
              " 'us': 130,\n",
              " 'director': 131,\n",
              " 'something': 132,\n",
              " 'end': 133,\n",
              " 'still': 134,\n",
              " 'back': 135,\n",
              " 'seems': 136,\n",
              " 'made': 137,\n",
              " 'those': 138,\n",
              " 'work': 139,\n",
              " 'theres': 140,\n",
              " 'makes': 141,\n",
              " 'before': 142,\n",
              " 'however': 143,\n",
              " 'now': 144,\n",
              " 'big': 145,\n",
              " 'years': 146,\n",
              " 'few': 147,\n",
              " 'world': 148,\n",
              " 'between': 149,\n",
              " 'every': 150,\n",
              " 'though': 151,\n",
              " 'seen': 152,\n",
              " 'better': 153,\n",
              " 'enough': 154,\n",
              " 'around': 155,\n",
              " 'take': 156,\n",
              " 'both': 157,\n",
              " 'performance': 158,\n",
              " 'why': 159,\n",
              " 'audience': 160,\n",
              " 'down': 161,\n",
              " 'going': 162,\n",
              " 'isnt': 163,\n",
              " 'same': 164,\n",
              " 'should': 165,\n",
              " 'gets': 166,\n",
              " 'role': 167,\n",
              " 'real': 168,\n",
              " 'may': 169,\n",
              " 'things': 170,\n",
              " 'your': 171,\n",
              " 'think': 172,\n",
              " 'last': 173,\n",
              " 'actually': 174,\n",
              " 'funny': 175,\n",
              " 'look': 176,\n",
              " 'own': 177,\n",
              " 'almost': 178,\n",
              " 'say': 179,\n",
              " 'thing': 180,\n",
              " 'nothing': 181,\n",
              " 'comedy': 182,\n",
              " 'fact': 183,\n",
              " 'although': 184,\n",
              " 'played': 185,\n",
              " 'thats': 186,\n",
              " 'right': 187,\n",
              " 'find': 188,\n",
              " 'john': 189,\n",
              " 'come': 190,\n",
              " 'since': 191,\n",
              " 'did': 192,\n",
              " 'script': 193,\n",
              " 'cast': 194,\n",
              " 'plays': 195,\n",
              " 'long': 196,\n",
              " 'young': 197,\n",
              " 'ever': 198,\n",
              " 'comes': 199,\n",
              " 'old': 200,\n",
              " 'actors': 201,\n",
              " 'original': 202,\n",
              " 'part': 203,\n",
              " 'show': 204,\n",
              " 'without': 205,\n",
              " 'acting': 206,\n",
              " 'each': 207,\n",
              " 'again': 208,\n",
              " 'star': 209,\n",
              " 'least': 210,\n",
              " 'lot': 211,\n",
              " 'point': 212,\n",
              " 'takes': 213,\n",
              " 'quite': 214,\n",
              " 'himself': 215,\n",
              " 'during': 216,\n",
              " 'away': 217,\n",
              " 'course': 218,\n",
              " 'goes': 219,\n",
              " 'cant': 220,\n",
              " 'minutes': 221,\n",
              " 'interesting': 222,\n",
              " 'effects': 223,\n",
              " 'three': 224,\n",
              " 'im': 225,\n",
              " 'year': 226,\n",
              " 'screen': 227,\n",
              " 'might': 228,\n",
              " 'family': 229,\n",
              " 'guy': 230,\n",
              " 'rather': 231,\n",
              " 'anything': 232,\n",
              " 'day': 233,\n",
              " 'far': 234,\n",
              " 'place': 235,\n",
              " 'must': 236,\n",
              " 'watch': 237,\n",
              " 'once': 238,\n",
              " 'our': 239,\n",
              " 'yet': 240,\n",
              " 'didnt': 241,\n",
              " 'seem': 242,\n",
              " 'always': 243,\n",
              " 'fun': 244,\n",
              " 'times': 245,\n",
              " 'instead': 246,\n",
              " 'trying': 247,\n",
              " 'bit': 248,\n",
              " 'special': 249,\n",
              " 'making': 250,\n",
              " 'give': 251,\n",
              " 'want': 252,\n",
              " 'sense': 253,\n",
              " 'job': 254,\n",
              " 'picture': 255,\n",
              " 'kind': 256,\n",
              " 'having': 257,\n",
              " 'wife': 258,\n",
              " 'set': 259,\n",
              " 'home': 260,\n",
              " 'probably': 261,\n",
              " 'series': 262,\n",
              " 'help': 263,\n",
              " 'along': 264,\n",
              " 'becomes': 265,\n",
              " 'pretty': 266,\n",
              " 'everything': 267,\n",
              " 'hollywood': 268,\n",
              " 'sure': 269,\n",
              " 'dialogue': 270,\n",
              " 'men': 271,\n",
              " 'together': 272,\n",
              " 'american': 273,\n",
              " 'woman': 274,\n",
              " 'actor': 275,\n",
              " 'become': 276,\n",
              " 'gives': 277,\n",
              " 'hard': 278,\n",
              " 'money': 279,\n",
              " 'given': 280,\n",
              " 'high': 281,\n",
              " 'black': 282,\n",
              " 'whole': 283,\n",
              " 'watching': 284,\n",
              " 'wants': 285,\n",
              " 'music': 286,\n",
              " 'got': 287,\n",
              " 'feel': 288,\n",
              " 'perhaps': 289,\n",
              " 'done': 290,\n",
              " 'especially': 291,\n",
              " 'death': 292,\n",
              " 'less': 293,\n",
              " 'next': 294,\n",
              " 'moments': 295,\n",
              " 'sex': 296,\n",
              " 'everyone': 297,\n",
              " 'play': 298,\n",
              " 'looks': 299,\n",
              " 'completely': 300,\n",
              " 'city': 301,\n",
              " 'looking': 302,\n",
              " 'reason': 303,\n",
              " 'whose': 304,\n",
              " 'horror': 305,\n",
              " 'shows': 306,\n",
              " 'rest': 307,\n",
              " 'until': 308,\n",
              " 'performances': 309,\n",
              " 'different': 310,\n",
              " 'simply': 311,\n",
              " 'james': 312,\n",
              " 'father': 313,\n",
              " 'friends': 314,\n",
              " 'ending': 315,\n",
              " 'couple': 316,\n",
              " 'put': 317,\n",
              " 'case': 318,\n",
              " 'several': 319,\n",
              " 'mind': 320,\n",
              " 'theyre': 321,\n",
              " 'evil': 322,\n",
              " 'left': 323,\n",
              " 'anyone': 324,\n",
              " 'michael': 325,\n",
              " 'night': 326,\n",
              " 'human': 327,\n",
              " 'shes': 328,\n",
              " 'small': 329,\n",
              " 'entire': 330,\n",
              " 'itself': 331,\n",
              " 'humor': 332,\n",
              " 'girl': 333,\n",
              " 'getting': 334,\n",
              " 'lost': 335,\n",
              " 'turns': 336,\n",
              " 'line': 337,\n",
              " 'main': 338,\n",
              " 'found': 339,\n",
              " 'use': 340,\n",
              " 'problem': 341,\n",
              " 'half': 342,\n",
              " 'begins': 343,\n",
              " 'true': 344,\n",
              " 'either': 345,\n",
              " 'stars': 346,\n",
              " 'mother': 347,\n",
              " 'soon': 348,\n",
              " 'ive': 349,\n",
              " 'unfortunately': 350,\n",
              " 'later': 351,\n",
              " 'final': 352,\n",
              " 'idea': 353,\n",
              " 'name': 354,\n",
              " 'someone': 355,\n",
              " 'school': 356,\n",
              " 'comic': 357,\n",
              " 'town': 358,\n",
              " 'thought': 359,\n",
              " 'wrong': 360,\n",
              " 'else': 361,\n",
              " 'based': 362,\n",
              " 'friend': 363,\n",
              " 'alien': 364,\n",
              " 'tries': 365,\n",
              " 'group': 366,\n",
              " 'second': 367,\n",
              " 'against': 368,\n",
              " 'house': 369,\n",
              " 'written': 370,\n",
              " 'david': 371,\n",
              " 'used': 372,\n",
              " 'sequence': 373,\n",
              " 'keep': 374,\n",
              " 'dead': 375,\n",
              " 'often': 376,\n",
              " 'certainly': 377,\n",
              " 'works': 378,\n",
              " 'relationship': 379,\n",
              " 'believe': 380,\n",
              " 'called': 381,\n",
              " 'named': 382,\n",
              " 'said': 383,\n",
              " 'despite': 384,\n",
              " 'playing': 385,\n",
              " 'behind': 386,\n",
              " 'head': 387,\n",
              " 'turn': 388,\n",
              " 'finally': 389,\n",
              " 'under': 390,\n",
              " 'war': 391,\n",
              " 'maybe': 392,\n",
              " 'doing': 393,\n",
              " 'tell': 394,\n",
              " 'days': 395,\n",
              " 'kids': 396,\n",
              " 'able': 397,\n",
              " 'finds': 398,\n",
              " 'seeing': 399,\n",
              " 'nice': 400,\n",
              " 'perfect': 401,\n",
              " 'youre': 402,\n",
              " 'past': 403,\n",
              " 'hand': 404,\n",
              " 'including': 405,\n",
              " 'book': 406,\n",
              " 'mr': 407,\n",
              " 'person': 408,\n",
              " 'shot': 409,\n",
              " 'lives': 410,\n",
              " 'boy': 411,\n",
              " 'run': 412,\n",
              " 'camera': 413,\n",
              " 'supposed': 414,\n",
              " 'live': 415,\n",
              " 'lines': 416,\n",
              " 'tv': 417,\n",
              " 'moment': 418,\n",
              " 'side': 419,\n",
              " 'directed': 420,\n",
              " 'need': 421,\n",
              " 'starts': 422,\n",
              " 'fight': 423,\n",
              " 'car': 424,\n",
              " 'entertaining': 425,\n",
              " 'summer': 426,\n",
              " 'style': 427,\n",
              " 'running': 428,\n",
              " 'game': 429,\n",
              " 'full': 430,\n",
              " 'worth': 431,\n",
              " 'dark': 432,\n",
              " 'worst': 433,\n",
              " 'face': 434,\n",
              " 'start': 435,\n",
              " 'upon': 436,\n",
              " 'try': 437,\n",
              " 'matter': 438,\n",
              " 'kevin': 439,\n",
              " 'others': 440,\n",
              " 'nearly': 441,\n",
              " 'hour': 442,\n",
              " 'care': 443,\n",
              " 'son': 444,\n",
              " 'opening': 445,\n",
              " 'throughout': 446,\n",
              " 'example': 447,\n",
              " 'exactly': 448,\n",
              " 'violence': 449,\n",
              " 'video': 450,\n",
              " 'early': 451,\n",
              " 'daughter': 452,\n",
              " 'major': 453,\n",
              " 'beautiful': 454,\n",
              " 'review': 455,\n",
              " 'problems': 456,\n",
              " 'sequences': 457,\n",
              " 'short': 458,\n",
              " 'wasnt': 459,\n",
              " 'version': 460,\n",
              " 'production': 461,\n",
              " 'title': 462,\n",
              " 'whos': 463,\n",
              " 'let': 464,\n",
              " 'robert': 465,\n",
              " 'obvious': 466,\n",
              " 'joe': 467,\n",
              " 'top': 468,\n",
              " 'classic': 469,\n",
              " 'screenplay': 470,\n",
              " 'already': 471,\n",
              " 'guys': 472,\n",
              " 'kill': 473,\n",
              " 'drama': 474,\n",
              " 'direction': 475,\n",
              " 'fine': 476,\n",
              " 'children': 477,\n",
              " 'eyes': 478,\n",
              " 'team': 479,\n",
              " 'order': 480,\n",
              " 'themselves': 481,\n",
              " 'roles': 482,\n",
              " 'simple': 483,\n",
              " 'hit': 484,\n",
              " 'knows': 485,\n",
              " 'question': 486,\n",
              " 'act': 487,\n",
              " 'sort': 488,\n",
              " 'supporting': 489,\n",
              " 'earth': 490,\n",
              " 'truly': 491,\n",
              " 'white': 492,\n",
              " 'deep': 493,\n",
              " 'save': 494,\n",
              " 'boring': 495,\n",
              " 'sometimes': 496,\n",
              " 'jack': 497,\n",
              " 'known': 498,\n",
              " 'women': 499,\n",
              " 'beginning': 500,\n",
              " 'wont': 501,\n",
              " 'scream': 502,\n",
              " 'coming': 503,\n",
              " 'hell': 504,\n",
              " 'jokes': 505,\n",
              " 'killer': 506,\n",
              " 'four': 507,\n",
              " 'attempt': 508,\n",
              " 'arent': 509,\n",
              " 'strong': 510,\n",
              " 'space': 511,\n",
              " 'tom': 512,\n",
              " 'happens': 513,\n",
              " 'body': 514,\n",
              " 'york': 515,\n",
              " 'room': 516,\n",
              " 'ends': 517,\n",
              " 'hope': 518,\n",
              " 'heart': 519,\n",
              " 'says': 520,\n",
              " 'jackie': 521,\n",
              " 'tells': 522,\n",
              " 'novel': 523,\n",
              " 'peter': 524,\n",
              " 'possible': 525,\n",
              " 'saw': 526,\n",
              " 'yes': 527,\n",
              " 'stupid': 528,\n",
              " 'quickly': 529,\n",
              " 'genre': 530,\n",
              " 'five': 531,\n",
              " 'lead': 532,\n",
              " 'extremely': 533,\n",
              " 'manages': 534,\n",
              " 'girls': 535,\n",
              " 'wonder': 536,\n",
              " 'murder': 537,\n",
              " 'particularly': 538,\n",
              " 'lee': 539,\n",
              " 'romantic': 540,\n",
              " 'level': 541,\n",
              " 'stop': 542,\n",
              " 'ship': 543,\n",
              " 'future': 544,\n",
              " 'appears': 545,\n",
              " 'career': 546,\n",
              " 'involving': 547,\n",
              " 'worse': 548,\n",
              " 'voice': 549,\n",
              " 'involved': 550,\n",
              " 'mostly': 551,\n",
              " 'thriller': 552,\n",
              " 'sets': 553,\n",
              " 'eventually': 554,\n",
              " 'police': 555,\n",
              " 'sound': 556,\n",
              " 'hours': 557,\n",
              " 'falls': 558,\n",
              " 'taking': 559,\n",
              " 'emotional': 560,\n",
              " 'attention': 561,\n",
              " 'result': 562,\n",
              " 'material': 563,\n",
              " 'dr': 564,\n",
              " 'ones': 565,\n",
              " 'elements': 566,\n",
              " 'planet': 567,\n",
              " 'hero': 568,\n",
              " 'close': 569,\n",
              " 'lack': 570,\n",
              " 'bring': 571,\n",
              " 'child': 572,\n",
              " 'meet': 573,\n",
              " 'whats': 574,\n",
              " 'piece': 575,\n",
              " 'note': 576,\n",
              " 'experience': 577,\n",
              " 'none': 578,\n",
              " 'fall': 579,\n",
              " 'van': 580,\n",
              " 'brother': 581,\n",
              " 'dog': 582,\n",
              " 'leads': 583,\n",
              " 'fiction': 584,\n",
              " 'fans': 585,\n",
              " 'living': 586,\n",
              " 'wild': 587,\n",
              " 'alone': 588,\n",
              " 'de': 589,\n",
              " 'enjoy': 590,\n",
              " 'theater': 591,\n",
              " 'battle': 592,\n",
              " 'obviously': 593,\n",
              " 'guess': 594,\n",
              " 'interest': 595,\n",
              " 'youll': 596,\n",
              " 'paul': 597,\n",
              " 'usually': 598,\n",
              " 'late': 599,\n",
              " 'feeling': 600,\n",
              " 'among': 601,\n",
              " 'taken': 602,\n",
              " 'laughs': 603,\n",
              " 'husband': 604,\n",
              " 'laugh': 605,\n",
              " 'parents': 606,\n",
              " 'george': 607,\n",
              " 'power': 608,\n",
              " 'aliens': 609,\n",
              " 'king': 610,\n",
              " 'mean': 611,\n",
              " 'happen': 612,\n",
              " 'attempts': 613,\n",
              " 'needs': 614,\n",
              " 'talent': 615,\n",
              " 'within': 616,\n",
              " 'number': 617,\n",
              " 'chance': 618,\n",
              " 'across': 619,\n",
              " 'single': 620,\n",
              " 'deal': 621,\n",
              " 'brothers': 622,\n",
              " 'chris': 623,\n",
              " 'talk': 624,\n",
              " 'williams': 625,\n",
              " 'forced': 626,\n",
              " 'feels': 627,\n",
              " 'wonderful': 628,\n",
              " 'success': 629,\n",
              " 'easy': 630,\n",
              " 'features': 631,\n",
              " 'god': 632,\n",
              " 'whether': 633,\n",
              " 'history': 634,\n",
              " 'expect': 635,\n",
              " 'killed': 636,\n",
              " 'words': 637,\n",
              " 'word': 638,\n",
              " 'feature': 639,\n",
              " 'premise': 640,\n",
              " 'television': 641,\n",
              " 'leave': 642,\n",
              " 'mission': 643,\n",
              " 'impressive': 644,\n",
              " 'science': 645,\n",
              " 'poor': 646,\n",
              " 'except': 647,\n",
              " 'form': 648,\n",
              " 'giving': 649,\n",
              " 'tale': 650,\n",
              " 'seemed': 651,\n",
              " 'recent': 652,\n",
              " 'call': 653,\n",
              " 'oscar': 654,\n",
              " 'meets': 655,\n",
              " 'disney': 656,\n",
              " 'basically': 657,\n",
              " 'score': 658,\n",
              " 'surprise': 659,\n",
              " 'serious': 660,\n",
              " 'apparently': 661,\n",
              " 'told': 662,\n",
              " 'important': 663,\n",
              " 'filmmakers': 664,\n",
              " 'crew': 665,\n",
              " 'entertainment': 666,\n",
              " 'released': 667,\n",
              " 'stuff': 668,\n",
              " 'somehow': 669,\n",
              " 'easily': 670,\n",
              " 'parts': 671,\n",
              " 'robin': 672,\n",
              " 'computer': 673,\n",
              " 'happy': 674,\n",
              " 'change': 675,\n",
              " 'brings': 676,\n",
              " 'art': 677,\n",
              " 'hilarious': 678,\n",
              " 'am': 679,\n",
              " 'whom': 680,\n",
              " 'ryan': 681,\n",
              " 'credits': 682,\n",
              " 'local': 683,\n",
              " 'events': 684,\n",
              " 'difficult': 685,\n",
              " 'remember': 686,\n",
              " 'went': 687,\n",
              " 'release': 688,\n",
              " 'working': 689,\n",
              " 'ago': 690,\n",
              " 'crime': 691,\n",
              " 'sequel': 692,\n",
              " 'certain': 693,\n",
              " 'wouldnt': 694,\n",
              " 'oh': 695,\n",
              " 'lets': 696,\n",
              " 'using': 697,\n",
              " 'id': 698,\n",
              " 'complete': 699,\n",
              " 'middle': 700,\n",
              " 'audiences': 701,\n",
              " 'cool': 702,\n",
              " 'william': 703,\n",
              " 'girlfriend': 704,\n",
              " 'due': 705,\n",
              " 'runs': 706,\n",
              " 'batman': 707,\n",
              " 'ben': 708,\n",
              " 'effective': 709,\n",
              " 'turned': 710,\n",
              " 'return': 711,\n",
              " 'viewer': 712,\n",
              " 'ill': 713,\n",
              " 'reality': 714,\n",
              " 'suspense': 715,\n",
              " 'smith': 716,\n",
              " 'flick': 717,\n",
              " 'quality': 718,\n",
              " 'presence': 719,\n",
              " 'popular': 720,\n",
              " 'uses': 721,\n",
              " 'anyway': 722,\n",
              " 'dramatic': 723,\n",
              " 'mystery': 724,\n",
              " 'personal': 725,\n",
              " 'begin': 726,\n",
              " 'surprisingly': 727,\n",
              " 'youve': 728,\n",
              " 'figure': 729,\n",
              " 'die': 730,\n",
              " 'decides': 731,\n",
              " 'writing': 732,\n",
              " 'viewers': 733,\n",
              " 'somewhat': 734,\n",
              " 'ways': 735,\n",
              " 'annoying': 736,\n",
              " 'absolutely': 737,\n",
              " 'similar': 738,\n",
              " 'previous': 739,\n",
              " 'blood': 740,\n",
              " 'business': 741,\n",
              " 'shots': 742,\n",
              " 'light': 743,\n",
              " 'came': 744,\n",
              " 'couldnt': 745,\n",
              " 'read': 746,\n",
              " 'strange': 747,\n",
              " 'gone': 748,\n",
              " 'excellent': 749,\n",
              " 'means': 750,\n",
              " 'former': 751,\n",
              " 'project': 752,\n",
              " 'latest': 753,\n",
              " 'sexual': 754,\n",
              " 'rich': 755,\n",
              " 'towards': 756,\n",
              " 'nor': 757,\n",
              " 'successful': 758,\n",
              " 'familiar': 759,\n",
              " 'visual': 760,\n",
              " 'amazing': 761,\n",
              " 'leaves': 762,\n",
              " 'intelligent': 763,\n",
              " 'following': 764,\n",
              " 'beyond': 765,\n",
              " 'leaving': 766,\n",
              " 'predictable': 767,\n",
              " 'romance': 768,\n",
              " 'wars': 769,\n",
              " 'present': 770,\n",
              " 'myself': 771,\n",
              " 'jim': 772,\n",
              " 'clear': 773,\n",
              " 'questions': 774,\n",
              " 'cut': 775,\n",
              " 'type': 776,\n",
              " 'starring': 777,\n",
              " 'kid': 778,\n",
              " 'definitely': 779,\n",
              " 'talking': 780,\n",
              " 'message': 781,\n",
              " 'add': 782,\n",
              " 'powerful': 783,\n",
              " 'party': 784,\n",
              " 'herself': 785,\n",
              " 'brilliant': 786,\n",
              " 'nature': 787,\n",
              " 'situation': 788,\n",
              " 'clever': 789,\n",
              " 'secret': 790,\n",
              " 'create': 791,\n",
              " 'opens': 792,\n",
              " 'stories': 793,\n",
              " 'felt': 794,\n",
              " 'red': 795,\n",
              " 'giant': 796,\n",
              " 'office': 797,\n",
              " 'villain': 798,\n",
              " 'usual': 799,\n",
              " 'straight': 800,\n",
              " 'third': 801,\n",
              " 'smart': 802,\n",
              " 'actress': 803,\n",
              " 'cinema': 804,\n",
              " 'company': 805,\n",
              " 'scary': 806,\n",
              " 'cop': 807,\n",
              " 'bunch': 808,\n",
              " 'age': 809,\n",
              " 'learn': 810,\n",
              " 'doubt': 811,\n",
              " 'prison': 812,\n",
              " 'bill': 813,\n",
              " 'large': 814,\n",
              " 'thinking': 815,\n",
              " 'solid': 816,\n",
              " 'move': 817,\n",
              " 'rock': 818,\n",
              " 'water': 819,\n",
              " 'bob': 820,\n",
              " 'follows': 821,\n",
              " 'saying': 822,\n",
              " 'million': 823,\n",
              " 'jones': 824,\n",
              " 'seriously': 825,\n",
              " 'writer': 826,\n",
              " 'effect': 827,\n",
              " 'potential': 828,\n",
              " 'america': 829,\n",
              " 'huge': 830,\n",
              " 'near': 831,\n",
              " 'plan': 832,\n",
              " 'unlike': 833,\n",
              " 'general': 834,\n",
              " 'animated': 835,\n",
              " 'realize': 836,\n",
              " 'likely': 837,\n",
              " 'follow': 838,\n",
              " 'perfectly': 839,\n",
              " 'motion': 840,\n",
              " 'understand': 841,\n",
              " 'decent': 842,\n",
              " 'martin': 843,\n",
              " 'took': 844,\n",
              " 'immediately': 845,\n",
              " 'mark': 846,\n",
              " 'moving': 847,\n",
              " 'subject': 848,\n",
              " 'married': 849,\n",
              " 'enjoyable': 850,\n",
              " 'sam': 851,\n",
              " 'happened': 852,\n",
              " 'heard': 853,\n",
              " 'created': 854,\n",
              " 'agent': 855,\n",
              " 'stay': 856,\n",
              " 'filled': 857,\n",
              " 'above': 858,\n",
              " 'th': 859,\n",
              " 'fails': 860,\n",
              " 'country': 861,\n",
              " 'merely': 862,\n",
              " 'points': 863,\n",
              " 'sweet': 864,\n",
              " 'exciting': 865,\n",
              " 'force': 866,\n",
              " 'slow': 867,\n",
              " 'overall': 868,\n",
              " 'break': 869,\n",
              " 'wanted': 870,\n",
              " 'escape': 871,\n",
              " 'bruce': 872,\n",
              " 'ultimately': 873,\n",
              " 'neither': 874,\n",
              " 'appear': 875,\n",
              " 'dream': 876,\n",
              " 'impossible': 877,\n",
              " 'private': 878,\n",
              " 'directors': 879,\n",
              " 'brought': 880,\n",
              " 'richard': 881,\n",
              " 'mess': 882,\n",
              " 'inside': 883,\n",
              " 'trouble': 884,\n",
              " 'r': 885,\n",
              " 'wedding': 886,\n",
              " 'favorite': 887,\n",
              " 'tim': 888,\n",
              " 'murphy': 889,\n",
              " 'liked': 890,\n",
              " 'fan': 891,\n",
              " 'otherwise': 892,\n",
              " 'musical': 893,\n",
              " 'various': 894,\n",
              " 'scott': 895,\n",
              " 'trek': 896,\n",
              " 'particular': 897,\n",
              " 'pay': 898,\n",
              " 'political': 899,\n",
              " 'keeps': 900,\n",
              " 'dumb': 901,\n",
              " 'ten': 902,\n",
              " 'situations': 903,\n",
              " 'steve': 904,\n",
              " 'chase': 905,\n",
              " 'talented': 906,\n",
              " 'minute': 907,\n",
              " 'harry': 908,\n",
              " 'members': 909,\n",
              " 'spend': 910,\n",
              " 'element': 911,\n",
              " 'truth': 912,\n",
              " 'society': 913,\n",
              " 'studio': 914,\n",
              " 'bond': 915,\n",
              " 'effort': 916,\n",
              " 'focus': 917,\n",
              " 'silly': 918,\n",
              " 'slightly': 919,\n",
              " 'earlier': 920,\n",
              " 'rating': 921,\n",
              " 'biggest': 922,\n",
              " 'open': 923,\n",
              " 'drug': 924,\n",
              " 'offers': 925,\n",
              " 'showing': 926,\n",
              " 'havent': 927,\n",
              " 'purpose': 928,\n",
              " 'cannot': 929,\n",
              " 'park': 930,\n",
              " 'memorable': 931,\n",
              " 'soundtrack': 932,\n",
              " 'eye': 933,\n",
              " 'fast': 934,\n",
              " 'frank': 935,\n",
              " 'totally': 936,\n",
              " 'mars': 937,\n",
              " 'cold': 938,\n",
              " 'english': 939,\n",
              " 'view': 940,\n",
              " 'ideas': 941,\n",
              " 'gun': 942,\n",
              " 'state': 943,\n",
              " 'subplot': 944,\n",
              " 'aspect': 945,\n",
              " 'wait': 946,\n",
              " 'ask': 947,\n",
              " 'government': 948,\n",
              " 'credit': 949,\n",
              " 'box': 950,\n",
              " 'eddie': 951,\n",
              " 'waste': 952,\n",
              " 'constantly': 953,\n",
              " 'actual': 954,\n",
              " 'entirely': 955,\n",
              " 'hands': 956,\n",
              " 'l': 957,\n",
              " 'law': 958,\n",
              " 'fear': 959,\n",
              " 'british': 960,\n",
              " 'moves': 961,\n",
              " 'terrible': 962,\n",
              " 'e': 963,\n",
              " 'gave': 964,\n",
              " 'west': 965,\n",
              " 'convincing': 966,\n",
              " 'ability': 967,\n",
              " 'u': 968,\n",
              " 'thinks': 969,\n",
              " 'spent': 970,\n",
              " 'ridiculous': 971,\n",
              " 'female': 972,\n",
              " 'typical': 973,\n",
              " 'cinematography': 974,\n",
              " 'atmosphere': 975,\n",
              " 'setting': 976,\n",
              " 'lots': 977,\n",
              " 'animation': 978,\n",
              " 'carter': 979,\n",
              " 'air': 980,\n",
              " 'fairly': 981,\n",
              " 'control': 982,\n",
              " 'background': 983,\n",
              " 'suddenly': 984,\n",
              " 'killing': 985,\n",
              " 'expected': 986,\n",
              " 'depth': 987,\n",
              " 'tension': 988,\n",
              " 'sees': 989,\n",
              " 'sit': 990,\n",
              " 'greatest': 991,\n",
              " 'critics': 992,\n",
              " 'army': 993,\n",
              " 'humans': 994,\n",
              " 'complex': 995,\n",
              " 'beauty': 996,\n",
              " 'brief': 997,\n",
              " 'violent': 998,\n",
              " 'amusing': 999,\n",
              " 'dull': 1000,\n",
              " ...}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token = Tokenizer()\n",
        "token.fit_on_texts(data[\"text\"])\n",
        "pop = token.word_index\n",
        "pop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pglL5mN_mKN"
      },
      "outputs": [],
      "source": [
        "#data[\"split_text\"] = data['text'].str.lower().str.split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8ccQdtO_mKN",
        "outputId": "4e0f04f0-ebf2-43e0-a9ab-11a31882b500"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       [2499, 181, 1, 4195, 6, 289, 26, 4, 1, 62, 372...\n",
              "1       [98, 2275, 8122, 6, 2, 1302, 2011, 17, 6, 71, ...\n",
              "2       [18, 174, 679, 2, 891, 4, 1, 202, 47, 38, 1497...\n",
              "3       [2, 27, 186, 68, 10, 1048, 1988, 46, 10, 1, 10...\n",
              "4       [61, 65, 1967, 6, 79, 101, 7, 26, 24, 2859, 15...\n",
              "                              ...                        \n",
              "1995    [1974, 43, 2, 14919, 46799, 7, 1, 4917, 2243, ...\n",
              "1996    [23, 144, 1, 6233, 4, 1, 23074, 4, 1293, 312, ...\n",
              "1997    [25627, 11, 1, 9732, 3671, 4, 101, 8, 621, 11,...\n",
              "1998    [3, 144, 1, 19667, 1448, 1489, 427, 4, 1225, 3...\n",
              "1999    [3082, 196, 495, 3, 50, 1634, 528, 3082, 490, ...\n",
              "Name: token_text, Length: 2000, dtype: object"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"token_text\"] = token.texts_to_sequences(data[\"text\"])\n",
        "data[\"token_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyk_ImDK_mKN"
      },
      "source": [
        "Select a review length L that 70% of the reviews have a length below it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltHHwlna_mKO",
        "outputId": "753080cc-236a-49f7-9fda-84f21d16ef5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "737"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "L = int(np.percentile(review_len, 70))\n",
        "L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZuM2sp8_mKO"
      },
      "source": [
        "Then, truncate reviews longer than L words and zero-pad reviews shorter than L so that all texts (= data points) are of length L.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO_5F7eM_mKP",
        "outputId": "361d2633-ea88-40d9-a7c7-81bb09f5aa82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       [109, 1141, 11933, 1194, 4196, 2, 3737, 3641, ...\n",
              "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2       [285, 5, 129, 5, 5, 276, 42, 9269, 36, 17378, ...\n",
              "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "                              ...                        \n",
              "1995    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1996    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1997    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1998    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1999    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "Name: pad_text, Length: 2000, dtype: object"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"pad_text\"] = list(pad_sequences(data[\"token_text\"], maxlen = L))\n",
        "data[\"pad_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-Ao6dRu_mKQ"
      },
      "source": [
        "## Word Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or7kdfd9_mKQ"
      },
      "source": [
        "### i. Use tokenized text as inputs to a deep neural network.\n",
        "However, a recent breakthrough in NLP suggests that more sophisticated representations of text yield better results. These sophisticated representations are called word embeddings. \\Word embedding is a term used for representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.\"4. Most deep learning modules (including Keras) provide a convenient way to convert positive integer representations of words into a word embedding by an \\Embedding layer.\" The layer accepts arguments that define the mapping of words into embeddings,including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value). The layer also allows you to specify the dimension for each word vector, called the \\output dimension.\" We would like to use a word embedding layer for this project. Assume that we are interested in the top 5,000 words. This means that in each integer sequence that represents each document, we set to zero those integers that represent words\n",
        "that are not among the top 5,000 words in the document.5 If you feel more adventurous, use all the words that appear in this corpus. Choose the length of the embedding vector for each word to be 32. Hence, each document is represented as a 32 Ã— L matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW0uwv2R_mKR"
      },
      "source": [
        "### ii. Flatten the matrix of each document to a vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIck4HIe_mKS"
      },
      "outputs": [],
      "source": [
        "input_text = np.array(list(data[\"pad_text\"]))\n",
        "input_text[input_text >= 5000] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga5NDqSA_mKS",
        "outputId": "cd3145f0-40ae-468c-c980-f8b778848c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method Model.summary of <tensorflow.python.keras.engine.sequential.Sequential object at 0x160e69df0>>\n",
            "[array([[-0.03551153, -0.0312135 ,  0.02762629, ..., -0.00306189,\n",
            "         0.03902471, -0.01229275],\n",
            "       [ 0.04268488, -0.04910192, -0.04501715, ...,  0.00510366,\n",
            "         0.04548797, -0.00195887],\n",
            "       [ 0.03849134,  0.03062275,  0.03373922, ...,  0.01225618,\n",
            "        -0.02856541,  0.0476712 ],\n",
            "       ...,\n",
            "       [ 0.02141747,  0.04477562, -0.00798661, ..., -0.0272519 ,\n",
            "         0.02140928,  0.04162479],\n",
            "       [ 0.03989843,  0.04823219, -0.00834541, ..., -0.02606398,\n",
            "        -0.00012935, -0.02234277],\n",
            "       [-0.01735542,  0.02892314,  0.02645124, ..., -0.00246954,\n",
            "        -0.02422092,  0.02956215]], dtype=float32)]\n",
            "[[ 0.03333518 -0.02617295 -0.02261096 ... -0.04879908 -0.02016357\n",
            "  -0.00410762]\n",
            " [-0.03551153 -0.0312135   0.02762629 ... -0.00327522 -0.00395964\n",
            "   0.02752754]\n",
            " [-0.0490008  -0.04394411 -0.0189552  ... -0.00306189  0.03902471\n",
            "  -0.01229275]\n",
            " ...\n",
            " [-0.03551153 -0.0312135   0.02762629 ...  0.0123669   0.03612501\n",
            "  -0.03114016]\n",
            " [-0.03551153 -0.0312135   0.02762629 ... -0.01558664  0.01954446\n",
            "  -0.01011112]\n",
            " [-0.03551153 -0.0312135   0.02762629 ... -0.04458426  0.02998788\n",
            "  -0.02043418]]\n"
          ]
        }
      ],
      "source": [
        "#resource: https://www.cnblogs.com/Renyi-Fan/p/13809918.html\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 32, input_length = L))\n",
        "model.add(Flatten())\n",
        "model.compile('rmsprop', 'mse')\n",
        "print(model.summary)\n",
        "\n",
        "embed_text = model.predict(input_text)\n",
        "print(model.layers[0].get_weights())\n",
        "print(embed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKswCCgl_mKl"
      },
      "source": [
        "## Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3FZtpM1_mKo",
        "outputId": "0ac47a5b-914f-4ea9-bd5d-5c96f5697cce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-5e7af8508837>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Data[\"emb_text\"] = list(embed_text)\n",
            "<ipython-input-18-5e7af8508837>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Data[\"class\"] = Data[\"class\"].replace(-1, 0)\n"
          ]
        }
      ],
      "source": [
        "Data = data[[\"num\", \"class\"]]\n",
        "Data[\"emb_text\"] = list(embed_text)\n",
        "Data[\"class\"] = Data[\"class\"].replace(-1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vJvxUr8_mKq"
      },
      "outputs": [],
      "source": [
        "Train = Data[Data[\"num\"] < 700]\n",
        "Test = Data[Data[\"num\"] >= 700]\n",
        "X_tr = np.array(list(Train[\"emb_text\"]))\n",
        "X_t = np.array(list(Test[\"emb_text\"]))\n",
        "Y_tr = Train[\"class\"]\n",
        "Y_t = Test[\"class\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrym5O8m_mKr"
      },
      "source": [
        "### i. Train a MLP with three (dense) hidden layers each of which has 50 ReLUs and one output layer with a single sigmoid neuron. Use a dropout rate of 20% for the first layer and 50% for the other layers. Use ADAM optimizer and binary cross entropy loss (which is equivalent to having a softmax in the output). To avoid overfitting, just set the number of epochs as 2. Use a batch size of 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrAY4nck_mKr",
        "outputId": "af317224-db9b-4c22-ba4b-13033d744691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "140/140 [==============================] - 2s 9ms/step - loss: 0.7053 - accuracy: 0.4943\n",
            "Epoch 2/2\n",
            "140/140 [==============================] - 1s 9ms/step - loss: 0.6841 - accuracy: 0.5241\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1618ebd00>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# resource: https://www.jianshu.com/p/d121ae396130?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(50, activation = 'relu', input_dim = 23584))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(50, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
        "model.fit(X_tr, Y_tr, epochs = 2, batch_size = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzQEmEap_mKr"
      },
      "source": [
        "### ii. Report the train and test accuracies of this model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmBAq7GJ_mKs",
        "outputId": "0a6ce262-8253-4e9f-8956-ae9e8c29880e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 0s 1ms/step - loss: 0.6883 - accuracy: 0.5533\n",
            "test_score:  0.6208227872848511\n",
            "140/140 [==============================] - 0s 1ms/step - loss: 0.6269 - accuracy: 0.6829\n",
            "test_score:  0.6548685431480408\n"
          ]
        }
      ],
      "source": [
        "test_score = model.evaluate(X_t, Y_t, batch_size = 10)\n",
        "print(\"test_score: \", mean(test_score))\n",
        "train_score = model.evaluate(X_tr, Y_tr, batch_size = 10)\n",
        "print(\"test_score: \", mean(train_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYpF8sCi_mKs"
      },
      "source": [
        "## One-Dimensional Convolutional Neural Network:\n",
        "Although CNNs are mainly used for image data, they can also be applied to text data, as text also has adjacency information. Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urvdPozh_mKv"
      },
      "source": [
        "### i. After the embedding layer, insert a Conv1D layer. This convolutional layer has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded word representations 3 vector elements of the word embedding at a time. The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geSLQu-i_mKw",
        "outputId": "9b873732-bd89-4dd5-fc76-addb3218e0bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method Model.summary of <tensorflow.python.keras.engine.sequential.Sequential object at 0x17c918a00>>\n",
            "[array([[-0.03625785, -0.00394065, -0.01474299, ...,  0.03426755,\n",
            "         0.03313489, -0.03360778],\n",
            "       [ 0.0382765 ,  0.00851555, -0.02150942, ...,  0.0178039 ,\n",
            "         0.01025463, -0.01649923],\n",
            "       [ 0.00754521,  0.04487438,  0.00597467, ..., -0.03997531,\n",
            "        -0.01019354,  0.04177039],\n",
            "       ...,\n",
            "       [-0.048969  ,  0.03454694, -0.00401114, ...,  0.02102664,\n",
            "         0.00111048,  0.02649759],\n",
            "       [-0.03555398, -0.04463471, -0.0326045 , ..., -0.02369057,\n",
            "        -0.0470906 , -0.02861959],\n",
            "       [-0.03839446,  0.02004881,  0.02382291, ..., -0.01642703,\n",
            "        -0.0390697 , -0.02226261]], dtype=float32)]\n",
            "[[ 1.16310893e-02  1.59416627e-02  4.05392908e-02 ...  2.37603653e-02\n",
            "   2.84849163e-02 -9.02268756e-03]\n",
            " [ 1.15169855e-02  4.81385097e-04  8.76351260e-03 ...  1.72630902e-02\n",
            "   2.32951529e-02  9.26091988e-03]\n",
            " [-3.18915071e-03  5.93612343e-02  2.09271610e-02 ...  4.64434028e-02\n",
            "  -8.77431966e-03  4.62909080e-02]\n",
            " ...\n",
            " [ 1.15169855e-02  4.81385097e-04  8.76351260e-03 ... -3.15948762e-03\n",
            "  -2.63488218e-02  2.62491424e-02]\n",
            " [ 1.15169855e-02  4.81385097e-04  8.76351260e-03 ...  3.67776155e-02\n",
            "   2.89064739e-02 -2.69080908e-03]\n",
            " [ 1.15169855e-02  4.81385097e-04  8.76351260e-03 ...  3.42333689e-03\n",
            "   4.80349222e-03 -6.62009697e-05]]\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(5000, 32, input_length = L))\n",
        "model.add(Conv1D(32,3))\n",
        "model.add(MaxPooling1D(pool_size=2, strides=2))\n",
        "model.add(Flatten())\n",
        "model.compile('rmsprop', 'mse')\n",
        "print(model.summary)\n",
        "\n",
        "embed_text = model.predict(input_text)\n",
        "print(model.layers[0].get_weights())\n",
        "print(embed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQyNHpBn_mKy",
        "outputId": "97bb93f2-0496-4d18-f863-55bde4838c05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-2e2d5b954f82>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Data1[\"emb_text\"] = list(embed_text)\n",
            "<ipython-input-23-2e2d5b954f82>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Data1[\"class\"] = Data1[\"class\"].replace(-1, 0)\n"
          ]
        }
      ],
      "source": [
        "Data1 = data[[\"num\", \"class\"]]\n",
        "Data1[\"emb_text\"] = list(embed_text)\n",
        "Data1[\"class\"] = Data1[\"class\"].replace(-1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujtEne54_mKy"
      },
      "outputs": [],
      "source": [
        "Train = Data1[Data1[\"num\"] < 700]\n",
        "Test = Data1[Data1[\"num\"] >= 700]\n",
        "X_tr = np.array(list(Train[\"emb_text\"]))\n",
        "X_t = np.array(list(Test[\"emb_text\"]))\n",
        "Y_tr = Train[\"class\"]\n",
        "Y_t = Test[\"class\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRLAI8Cl_mKz",
        "outputId": "9423e344-9c1d-4905-d171-453cd74cfdd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "140/140 [==============================] - 1s 6ms/step - loss: 0.7036 - accuracy: 0.5212\n",
            "Epoch 2/2\n",
            "140/140 [==============================] - 1s 6ms/step - loss: 0.6949 - accuracy: 0.5070\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x17c647340>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(50, activation = 'relu', input_dim = 11744))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(50, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
        "model.fit(X_tr, Y_tr, epochs = 2, batch_size = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaS1F6Pb_mKz"
      },
      "source": [
        "### ii. Report the train and test accuracies of this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwYKDMIo_mKz",
        "outputId": "b983aa75-0b2d-44db-c99a-134f2f076f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 0s 898us/step - loss: 0.6886 - accuracy: 0.5567\n",
            "test_score:  0.6226412951946259\n",
            "140/140 [==============================] - 0s 987us/step - loss: 0.6824 - accuracy: 0.5714\n",
            "test_score:  0.6268921792507172\n"
          ]
        }
      ],
      "source": [
        "test_score = model.evaluate(X_t, Y_t, batch_size = 10)\n",
        "print(\"test_score: \", mean(test_score))\n",
        "train_score = model.evaluate(X_tr, Y_tr, batch_size = 10)\n",
        "print(\"test_score: \", mean(train_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zArcfylf_mK0"
      },
      "source": [
        "## Long Short-Term Memory Recurrent Neural Network\n",
        "The structure of the LSTM we are going to use is shown in the following figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHGab0oW_mK1"
      },
      "source": [
        "### i. Each word is represented to LSTM as a vector of 32 elements and the LSTM is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both LSTM and the dense layer. Train the model using 10-50 epochs and batch size of 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Db69ALe_mK3"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(5000, 32, input_length = L))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
        "#model.fit(X_tr, Y_tr, epochs = 10, batch_size = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAGpkpHY_mK4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46dbuA2U_mK4",
        "outputId": "7cbfcf8b-2843-4535-eb3d-ab6c8c961555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 737) for input KerasTensor(type_spec=TensorSpec(shape=(None, 737), dtype=tf.float32, name='embedding_4_input'), name='embedding_4_input', description=\"created by layer 'embedding_4_input'\"), but it was called on an input with incompatible shape (10, 11744).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 737) for input KerasTensor(type_spec=TensorSpec(shape=(None, 737), dtype=tf.float32, name='embedding_4_input'), name='embedding_4_input', description=\"created by layer 'embedding_4_input'\"), but it was called on an input with incompatible shape (10, 11744).\n",
            "60/60 [==============================] - 21s 333ms/step - loss: 0.6956 - accuracy: 0.1632\n",
            "test_score:  0.5965770184993744\n",
            "140/140 [==============================] - 48s 344ms/step - loss: 0.6932 - accuracy: 0.5000\n",
            "test_score:  0.5965765416622162\n"
          ]
        }
      ],
      "source": [
        "test_score = model.evaluate(X_t, Y_t, batch_size = 10)\n",
        "print(\"test_score: \", mean(test_score))\n",
        "train_score = model.evaluate(X_tr, Y_tr, batch_size = 10)\n",
        "print(\"test_score: \", mean(train_score))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "70317e835dd10303872b9e388b9a8744218191996abeafb96f0408864417b075"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}